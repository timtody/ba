\section{Materials and Methods}\label{materials_and_methods}
\subsection{Generatives Modell für Stimuli}
Um die Effekte von Rekursion auf das klassifizieren von okkludierten Objekten zu untersuchen, wurde ein möglichst einfaches Grundproblem betrachtet. Die Klassifizierung von Ziffern ist ein gut untersuchtes Machine Learning Problem, in dem übermenschliche Performance erreicht werden kann. Allgemein gilt das sehr ähnliche MNIST-Datenset gemeinhin als das "Hello world!" des Machine Learning. Durch die Einfachheit der Aufgabe, können die Auswirkungen von Rekursion isoliert von anderen Herausforderungen betrachtet werden. 
\subsection{Models}
Um die Wirkungskraft von Rekursion zu untersuchen, wurden verschiedene CNNs mit variierenden Graden an Rekursion benutzt und systematisch miteinander verglichen. Die Nomenklatur richtet sich am Paper von Spoerer und Kriegeskorte aus. Als Grundlage dient eine Standard feed-forward Architektur (B) mit reinen 'bottom-up' Verbindungen. Da diese jedoch in der Anzahl der Parameter und der Anzahl der durchgeführten Konvolutionen gegenüber seinen Rekursiven Varianten unterlegen ist, wurden zum Vergleich zusätzlich Modelle entwickelt, die in den entsprechenden Domänen angepasst wurden. Einerseits wurde die Größe der Konvolutionskernel angepasst und somit die Größe der erlernbaren Features. Andererseits wurde die Anzahl der Konvolutionen erhöht, die Gewichte in den zusätzlichen Konvolutionen jedoch mit den anderen Konvolutionen geteilt. Somit kann die Anzahl der Faltungsoperationen vergleichbar gemacht werden, ohne die freien Parameter zu erhöhen. Die Archtitekturen werden im Folgenden BK respektive BKC genannt. 
\subsubsection{Implementation}
Apart from BKC, all models consist of two convolution layers. All bottom up convolutions are implemented as standard convolutions with 1x1 stride and zero padding, which leads to the output pictures being the same size as the input. The output of the convolution is then fed into a parameterized version of the Rectified Linear Unit activation function (PReLU). PReLU works as a generalized form of the ReLU activation function as it controls the output for negative values with a slope which can be learned. $$f(x_i) = \begin{cases}
x_i,\ \ \ if\ y_i > 0\\    
a_iy_i,\ if\ y_i \leq 0
\end{cases}$$ If the slope parameter is zero, PReLU results in standard ReLU. If the parameter is a small positive number, PReLU equals Leaky ReLU. The output of the PReLU activation is then normalized with local response normalization (LRN), which tries to account for lateral inhibition which was found in brains. LRN therefore dampens responses which are uniformly large in any given local neighborhood and strengthens activations which are relatively larger than their surroundings and so imposes sparse activations. After normalization, the image is fed into a max-pooling layer with 2x2 stride and a pooling window of size 2x2, thus reducing the image size by half in each dimension. This whole process is repeated for each of the convolutions. After the second pooling, the image therefore is 8x8 in size and is flattened to a 1x64 vector before being fed into a readout layer, which maps the input to {\tiny }1x10 vector. The sigmoid function is then applied to the final output, yielding values from 0 to 1 which can be interpreted as the probability that each of the responding targets is present in the given input picture.
\\
\subsubsection{Recurrent Convolution Layers}
The heart of this work are recurrent convolution layers (RCL) whose effect on occluded image recognition is to be invetigated. We denote the input of layer l at timestep t with $I_{l, t}$ and use a vectorized format which contains values across feature maps. The input at layer 0 e.g. $I_{0, t}$ is defined as the input picture. The preactivation of B is given by\begin{align*}
	p_{l, t, m} = K_{l, m} * I_{l-1, t} + b_{l, m}
\end{align*}
in which $*$ represents the convolution operation, $K_{l, m}$ and $b_{l, m}$ being the convolution kernel and the bias respectively, each for layer l and feature map m.\\
BL has additional lateral connections and the preactivation is given by \begin{align*}
	p_{l, t, m} = K^b_{l, m} * I_{l-1, t} + K^l_{l, m} * I_{l, t-1}  + b_{l, m} 	
\end{align*}
where the output of layer l at time step t-1 is convolved with the lateral convolution kernel $K^l_{l, m}$ and added to the preactivation of layer l at time step t. Following the same principles, we can construct the preactivation of BT which is given as
\begin{align*}
	p_{l, t, m} = K^b_{l, m} * I_{l-1, t} + K^t_{l, m} * I_{l-1, t}  + b_{l, m}
\end{align*}
When combining lateral and top down connections, we are left with BLT which is the full recursive model, yielding a preactivation of
\begin{align*}
	p_{t, l, m} = K^b_{l, m} * I_{l-1, t} + K^t_{l, m} * I_{l-1, t}  + K^l_{l, m} * I_{l, t-1} + b_{l, m}
\end{align*}
Each preactivation is then fed into a ReLU and a Local Response Normalization Layer. ReLU is defined as $$r_{t, l, m} = max\{0, p_{t, l, m}\}$$
and Local Response Normalization (LRN) (Krizhevsky et al., \cite{NIPS2012_4824}) is given by 
\begin{align*}
	lrn(x) = x\left(c+\alpha\sum_{k'=max(0, k-n/2}^{min(n-1, k+n/2)}x^2\right)^{-\beta}
\end{align*}
with n = 5, c = 1, $\alpha$ = 10-4, and $\beta$ = 0.5 with the sum over n adjacent kernel maps at the same spatial position. Even though ReLU activations do not require for input normalization to prevent saturation, LRN seems to aid generalization by imposing a competition among adjacent neurons by simulating a concept of lateral inhibition. Hence the output of layer l at time step t is 
\begin{align*}
	\omega_{l, t} = lrn(r(p_{l, t, m}))
\end{align*}
\