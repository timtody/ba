%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Mak at 2011-07-01 23:12:31 +0200 


%% Saved with string encoding Unicode (UTF-8) 



@ARTICLE{10.3389/fpsyg.2017.01551,
  
AUTHOR={Spoerer, Courtney J. and McClure, Patrick and Kriegeskorte, Nikolaus},   
	 
TITLE={Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition},      
	
JOURNAL={Frontiers in Psychology},      
	
VOLUME={8},      

PAGES={1551},     
	
YEAR={2017},      
	  
URL={https://www.frontiersin.org/article/10.3389/fpsyg.2017.01551},       
	
DOI={10.3389/fpsyg.2017.01551},      
	
ISSN={1664-1078},   
   
ABSTRACT={Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and nonhuman primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, \emph{digit clutter} (where multiple target digits occlude one another) and \emph{digit debris} (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognising objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognise objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.}
}
@article{NIPS2012_4824,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}
@article{DICARLO2012415,
title = "How Does the Brain Solve Visual Object Recognition?",
journal = "Neuron",
volume = "73",
number = "3",
pages = "415 - 434",
year = "2012",
issn = "0896-6273",
doi = "https://doi.org/10.1016/j.neuron.2012.01.010",
url = "http://www.sciencedirect.com/science/article/pii/S089662731200092X",
author = "James J. DiCarlo and Davide Zoccolan and Nicole C. Rust",
abstract = "Mounting evidence suggests that ‘core object recognition,’ the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging from individual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal."
}
@Article{Sugase1999,
author={Sugase, Yasuko
and Yamane, Shigeru
and Ueno, Shoogo
and Kawano, Kenji},
title={Global and fine information coded by single neurons in the temporal visual cortex},
journal={Nature},
year={1999},
month={Aug},
day={26},
publisher={Macmillan Magazines Ltd. SN  -},
volume={400},
pages={869 EP  -},
url={http://dx.doi.org/10.1038/23703}
}

@book{hastie01statisticallearning,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}
@article{rumelhart1988learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive modeling},
  volume={5},
  number={3},
  pages={1},
  year={1988}
}
@article{rosenblatt1958perceptron,
  title={The perceptron: A probabilistic model for information storage and organization in the brain.},
  author={Rosenblatt, Frank},
  journal={Psychological review},
  volume={65},
  number={6},
  pages={386},
  year={1958},
  publisher={American Psychological Association}
}
@article{mcculloch1943logical,
  title={A logical calculus of the ideas immanent in nervous activity},
  author={McCulloch, Warren S and Pitts, Walter},
  journal={The bulletin of mathematical biophysics},
  volume={5},
  number={4},
  pages={115--133},
  year={1943},
  publisher={Springer}
}
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={IEEE}
}
@article{serre2013hierarchical,
  title={Hierarchical models of the visual system},
  author={Serre, Thomas},
  journal={Encyclopedia of computational neuroscience},
  pages={1--12},
  year={2013},
  publisher={Springer}
}
@article{hubel1962receptive,
  title={Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  author={Hubel, David H and Wiesel, Torsten N},
  journal={The Journal of physiology},
  volume={160},
  number={1},
  pages={106--154},
  year={1962},
  publisher={Wiley Online Library}
}
@article{matsugu2003subject,
  title={Subject independent facial expression recognition with robust face detection using a convolutional neural network},
  author={Matsugu, Masakazu and Mori, Katsuhiko and Mitari, Yusuke and Kaneda, Yuji},
  journal={Neural Networks},
  volume={16},
  number={5},
  pages={555--559},
  year={2003},
  publisher={Elsevier}
}
